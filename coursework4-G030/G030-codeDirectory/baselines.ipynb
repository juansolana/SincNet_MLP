{
  "cells": [
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn.model_selection import train_test_split, KFold, cross_val_predict\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3915310cf54dd876a970672a0b558715362cc542"
      },
      "cell_type": "code",
      "source": "import matplotlib.pyplot as plt\nimport xgboost\nfrom tqdm import tqdm\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import NuSVR\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error\nfrom scipy.stats import kurtosis, skew\n\n\n# Make pandas show more decimal places\npd.options.display.precision = 15",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "281b1744d7859c0203c118b8ae91cec24f6b3a27"
      },
      "cell_type": "markdown",
      "source": "Load the data and see how it looks like"
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true
      },
      "cell_type": "code",
      "source": "train = pd.read_csv('../input/train.csv', dtype={'acoustic_data': np.int16, 'time_to_failure': np.float32})",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "89def04b3bad7316a0909f9d703bceb7cd620298",
        "scrolled": true
      },
      "cell_type": "code",
      "source": "train.head(), train.shape",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "6bdcd7cdd22c55258fa48692743c6ce22bcc6c31"
      },
      "cell_type": "markdown",
      "source": "Create train set and target set in segments of *rows* size. 150,000 seems a good choice since test set files have that number of rows."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5318bc500ae3b2168224badb7eeb164dee64d3b0",
        "scrolled": true
      },
      "cell_type": "code",
      "source": "# Create a training file with simple derived features\n\nrows = 150000\nstride = 3750\nsegments = int(1 + np.floor((train.shape[0] - rows) / stride))\n\nX_train = pd.DataFrame(index=range(segments), dtype=np.float64,\n                       columns=['ave', 'std', 'max', 'min','q95','q99', 'q05','q01', 'kurtosis', 'variance', 'skew', 'median', 'mad', ])\ny_train = pd.DataFrame(index=range(segments), dtype=np.float64,\n                       columns=['time_to_failure'])\n\nlast_index = 0\nfor segment in tqdm(range(segments)):\n    seg = train.iloc[segment*stride:segment*stride+rows]\n    last_index = segment*stride+rows\n    x = seg['acoustic_data'].values\n    y = seg['time_to_failure'].values[-(int(rows/2)-1)]\n    \n    y_train.loc[segment, 'time_to_failure'] = y\n    \n    X_train.loc[segment, 'ave'] = x.mean()\n    X_train.loc[segment, 'std'] = x.std()\n    X_train.loc[segment, 'max'] = x.max()\n    X_train.loc[segment, 'min'] = x.min()\n    \n    X_train.loc[segment, 'q95'] = np.quantile(x,0.95)\n    X_train.loc[segment, 'q99'] = np.quantile(x,0.99)\n    X_train.loc[segment, 'q05'] = np.quantile(x,0.05)\n    X_train.loc[segment, 'q01'] = np.quantile(x,0.01)\n    \n    X_train.loc[segment, 'kurtosis'] = kurtosis(x, bias=False)\n    X_train.loc[segment, 'variance'] = np.var(x)\n    X_train.loc[segment, 'skew'] = skew(x)\n    \n    X_train.loc[segment, 'median'] = np.median(x)\n    X_train.loc[segment, 'mad'] = np.mean(np.abs(x - x.mean()))\n    \n    X_train.loc[segment, 'abs_mean'] = np.abs(x).mean()\n    X_train.loc[segment, 'abs_std'] = np.abs(x).std()\n    \n#     # Look at some chunkks of the current segment\n#     X_train.loc[segment, 'first_50k_ave'] = x[:50000].mean()\n#     X_train.loc[segment, 'first_50k_std'] = x[:50000].std()\n#     X_train.loc[segment, 'first_50k_max'] = x[:50000].max()\n#     X_train.loc[segment, 'first_50k_min'] = x[:50000].min()\n#     X_train.loc[segment, 'first_50k_q95'] = np.quantile(x[:50000],0.95)\n#     X_train.loc[segment, 'first_50k_q99'] = np.quantile(x[:50000],0.99)\n#     X_train.loc[segment, 'first_50k_q05'] = np.quantile(x[:50000],0.05)\n#     X_train.loc[segment, 'first_50k_q01'] = np.quantile(x[:50000],0.01)\n#     X_train.loc[segment, 'first_50k_kurtosis'] = kurtosis(x[:50000], bias=False)\n#     X_train.loc[segment, 'first_50k_variance'] = np.var(x[:50000])\n#     X_train.loc[segment, 'first_50k_skew'] = skew(x[:50000])    \n#     X_train.loc[segment, 'first_50k_median'] = np.median(x[:50000])\n#     X_train.loc[segment, 'first_50k_mad'] = np.mean(np.abs(x[:50000] - x[:50000].mean()))    \n#     X_train.loc[segment, 'first_50k_abs_mean'] = np.abs(x[:50000]).mean()\n#     X_train.loc[segment, 'first_50k_abs_std'] = np.abs(x[:50000]).std()\n    \n#     X_train.loc[segment, 'middle_50k_ave'] = x[50000:100000].mean()\n#     X_train.loc[segment, 'middle_50k_std'] = x[50000:100000].std()\n#     X_train.loc[segment, 'middle_50k_max'] = x[50000:100000].max()\n#     X_train.loc[segment, 'middle_50k_min'] = x[50000:100000].min()\n#     X_train.loc[segment, 'middle_50k_q95'] = np.quantile(x[50000:100000],0.95)\n#     X_train.loc[segment, 'middle_50k_q99'] = np.quantile(x[50000:100000],0.99)\n#     X_train.loc[segment, 'middle_50k_q05'] = np.quantile(x[50000:100000],0.05)\n#     X_train.loc[segment, 'middle_50k_q01'] = np.quantile(x[50000:100000],0.01)\n#     X_train.loc[segment, 'middle_50k_kurtosis'] = kurtosis(x[50000:100000], bias=False)\n#     X_train.loc[segment, 'middle_50k_variance'] = np.var(x[50000:100000])\n#     X_train.loc[segment, 'middle_50k_skew'] = skew(x[50000:100000])    \n#     X_train.loc[segment, 'middle_50k_median'] = np.median(x[50000:100000])\n#     X_train.loc[segment, 'middle_50k_mad'] = np.mean(np.abs(x[50000:100000] - x[50000:100000].mean()))    \n#     X_train.loc[segment, 'middle_50k_abs_mean'] = np.abs(x[50000:100000]).mean()\n#     X_train.loc[segment, 'middle_50k_abs_std'] = np.abs(x[50000:100000]).std()\n    \n#     X_train.loc[segment, 'last_50k_ave'] = x[100000:150000].mean()\n#     X_train.loc[segment, 'last_50k_std'] = x[100000:150000].std()\n#     X_train.loc[segment, 'last_50k_max'] = x[100000:150000].max()\n#     X_train.loc[segment, 'last_50k_min'] = x[100000:150000].min()\n#     X_train.loc[segment, 'last_50k_q95'] = np.quantile(x[100000:150000],0.95)\n#     X_train.loc[segment, 'last_50k_q99'] = np.quantile(x[100000:150000],0.99)\n#     X_train.loc[segment, 'last_50k_q05'] = np.quantile(x[100000:150000],0.05)\n#     X_train.loc[segment, 'last_50k_q01'] = np.quantile(x[100000:150000],0.01)\n#     X_train.loc[segment, 'last_50k_kurtosis'] = kurtosis(x[100000:150000], bias=False)\n#     X_train.loc[segment, 'last_50k_variance'] = np.var(x[100000:150000])\n#     X_train.loc[segment, 'last_50k_skew'] = skew(x[100000:150000])    \n#     X_train.loc[segment, 'last_50k_median'] = np.median(x[100000:150000])\n#     X_train.loc[segment, 'last_50k_mad'] = np.mean(np.abs(x[100000:150000] - x[100000:150000].mean()))    \n#     X_train.loc[segment, 'last_50k_abs_mean'] = np.abs(x[100000:150000]).mean()\n#     X_train.loc[segment, 'last_50k_abs_std'] = np.abs(x[100000:150000]).std()\n    \n#     X_train.loc[segment, 'last_bin_ave'] = x[145904:150000].mean()\n#     X_train.loc[segment, 'last_bin_std'] = x[145904:150000].std()\n#     X_train.loc[segment, 'last_bin_max'] = x[145904:150000].max()\n#     X_train.loc[segment, 'last_bin_min'] = x[145904:150000].min()\n#     X_train.loc[segment, 'last_bin_q95'] = np.quantile(x[145904:150000],0.95)\n#     X_train.loc[segment, 'last_bin_q99'] = np.quantile(x[145904:150000],0.99)\n#     X_train.loc[segment, 'last_bin_q05'] = np.quantile(x[145904:150000],0.05)\n#     X_train.loc[segment, 'last_bin_q01'] = np.quantile(x[145904:150000],0.01)\n#     X_train.loc[segment, 'last_bin_kurtosis'] = kurtosis(x[145904:150000], bias=False)\n#     X_train.loc[segment, 'last_bin_variance'] = np.var(x[145904:150000])\n#     X_train.loc[segment, 'last_bin_skew'] = skew(x[145904:150000])    \n#     X_train.loc[segment, 'last_bin_median'] = np.median(x[145904:150000])\n#     X_train.loc[segment, 'last_bin_mad'] = np.mean(np.abs(x[145904:150000] - x[100000:150000].mean()))    \n#     X_train.loc[segment, 'last_bin_abs_mean'] = np.abs(x[145904:150000]).mean()\n#     X_train.loc[segment, 'last_bin_abs_std'] = np.abs(x[145904:150000]).std()\n\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "ebf8a4dc20feff2f849dc10a2d1632ee94f1b3c3"
      },
      "cell_type": "markdown",
      "source": "Take a look at how the train set looks like. Then scale it and recheck shapes."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "01f12837a54492ae370bba73b2571a4be33cdfad",
        "scrolled": true
      },
      "cell_type": "code",
      "source": "X_train.shape\nX_train.describe()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8e8978bab674703eb29cf20a5f64c76573e28b08"
      },
      "cell_type": "code",
      "source": "X_train.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0715a23984c2fe3988f2a903a8335ba4ac8f5776"
      },
      "cell_type": "code",
      "source": "scaler = StandardScaler()\nscaler.fit(X_train)\nX_train_scaled = scaler.transform(X_train)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7d91d1c0ad7cd6fca790f010286b796b2a9e903f"
      },
      "cell_type": "code",
      "source": "X_train_scaled.shape, X_train_scaled[0:5]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "b4ea088c0d6c38d41a879a30c89c8e8e399f0027"
      },
      "cell_type": "markdown",
      "source": "## SVM\nLets train an SVM Regressor model with default values as one of our baseline models.\nPlot a scatter of its prediction vs actual value.\nPrint MAE Score."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "db6324515576fd3364e58d9af663511e21c6cb8d",
        "scrolled": false
      },
      "cell_type": "code",
      "source": "print ('SVM Training')\nsvm = NuSVR()\nsvm.fit(X_train_scaled, y_train.values.flatten())\nsvm_pred = svm.predict(X_train_scaled)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "41f5e05ad77d58d3deeb0d03170c8af97242d838"
      },
      "cell_type": "code",
      "source": "plt.figure(figsize=(6, 6))\nplt.scatter(y_train.values.flatten(), svm_pred)\nplt.xlim(0, 20)\nplt.ylim(0, 20)\nplt.xlabel('actual', fontsize=12)\nplt.ylabel('predicted', fontsize=12)\nplt.plot([(0, 0), (20, 20)], [(0, 0), (20, 20)])\nplt.title('SVM')\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f5f622f38cfdb7ca830f38d5c9f536ac86dfe3f4"
      },
      "cell_type": "code",
      "source": "svm_score = mean_absolute_error(y_train.values.flatten(), svm_pred)\nprint(f'Score: {svm_score:0.3f}')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "d0925a954f47e52861df15da93a7e64a7a4f6f64"
      },
      "cell_type": "markdown",
      "source": "## Random Forest\nLets train a Random Forest model with default values as one of our baseline models. Plot a scatter of its prediction vs actual value. Print MAE Score."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c5e5c8176b0bcc13bf9319efab8b3981eff394b4"
      },
      "cell_type": "code",
      "source": "print ('Random Forest Training')\nrf = RandomForestRegressor(n_estimators=100, criterion='mae')\nrf.fit(X_train_scaled, y_train.values.flatten())\nrf_pred = rf.predict(X_train_scaled)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3b4c68ff4b90d787d4d5bef36b375a20e960598b"
      },
      "cell_type": "code",
      "source": "plt.figure(figsize=(6, 6))\nplt.scatter(y_train.values.flatten(), rf_pred)\nplt.xlim(0, 20)\nplt.ylim(0, 20)\nplt.xlabel('actual', fontsize=12)\nplt.ylabel('predicted', fontsize=12)\nplt.plot([(0, 0), (20, 20)], [(0, 0), (20, 20)])\nplt.title('Random Forest')\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "03dcbc8283dd7a2639b7fc1fc3b2b3a8885bd30c"
      },
      "cell_type": "code",
      "source": "rf_score = mean_absolute_error(y_train.values.flatten(), rf_pred)\nprint(f'Score: {rf_score:0.3f}')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "4e138933ba967c2dfc9f84333155cc5a5e3a70e9"
      },
      "cell_type": "markdown",
      "source": "## XGBoost\nLets train an XGBoost model with default values as one of our baseline models.\nPlot a scatter of its prediction vs actual value.\nPrint MAE Score."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f470bb5fe693ab3586b9416ded1a47d91e654c6d"
      },
      "cell_type": "code",
      "source": "print ('XGBoost Training')\nxgb = xgboost.XGBRegressor(objective=\"reg:linear\", eval_metric='mae', n_jobs=4, )\nxgb.fit(X_train_scaled, y_train.values.flatten())\nxgb_pred = xgb.predict(X_train_scaled)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "aaad8933ad6bf5d8a68e522ec3e10efacccd39ad"
      },
      "cell_type": "code",
      "source": "plt.figure(figsize=(6, 6))\nplt.scatter(y_train.values.flatten(), xgb_pred)\nplt.xlim(0, 20)\nplt.ylim(0, 20)\nplt.xlabel('actual', fontsize=12)\nplt.ylabel('predicted', fontsize=12)\nplt.plot([(0, 0), (20, 20)], [(0, 0), (20, 20)])\nplt.title('XGBoost')\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2debd76c14dab5a6bb203387561a9825a14b7088"
      },
      "cell_type": "code",
      "source": "xgb_score = mean_absolute_error(y_train.values.flatten(), xgb_pred)\nprint(f'Score: {xgb_score:0.3f}')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "e9bb24752ad868f17bf8b1ef13208e5f3ee56635"
      },
      "cell_type": "markdown",
      "source": "## Test Models"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "865af4f82f38f162ba99e170973255b2e8ce5081"
      },
      "cell_type": "code",
      "source": "submission = pd.read_csv('../input/sample_submission.csv', index_col='seg_id')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "231f4b7de5be6261f559d7934664c30181aea85a"
      },
      "cell_type": "code",
      "source": "X_test = pd.DataFrame(columns=X_train.columns, dtype=np.float64, index=submission.index)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "9d44923f82398856244c4a0851da791bf16e7e62",
        "scrolled": true
      },
      "cell_type": "code",
      "source": "print ('Testing')\nfor seg_id in X_test.index:\n    seg = pd.read_csv('../input/test/' + seg_id + '.csv')\n    segments = int(np.floor(seg.shape[0] / rows))\n    for segment in range(segments):\n        test_seg = seg.iloc[segment*rows:segment*rows+rows]\n        x = test_seg['acoustic_data'].values\n\n        X_test.loc[seg_id, 'ave'] = x.mean()\n        X_test.loc[seg_id, 'std'] = x.std()\n        X_test.loc[seg_id, 'max'] = x.max()\n        X_test.loc[seg_id, 'min'] = x.min()\n        \n        X_test.loc[seg_id, 'q95'] = np.quantile(x,0.95)\n        X_test.loc[seg_id, 'q99'] = np.quantile(x,0.99)\n        X_test.loc[seg_id, 'q05'] = np.quantile(x,0.05)\n        X_test.loc[seg_id, 'q01'] = np.quantile(x,0.01)\n        \n        X_test.loc[seg_id, 'kurtosis'] = kurtosis(x, bias=False)\n        X_test.loc[seg_id, 'variance'] = np.var(x)\n#         X_test.loc[seg_id, 'abs_energy'] = np.dot(x, x)\n        X_test.loc[seg_id, 'skew'] = skew(x)\n        \n        X_test.loc[seg_id, 'median'] = np.median(x)\n        X_test.loc[seg_id, 'mad'] = np.mean(np.abs(x - x.mean()))\n\n        X_test.loc[seg_id, 'abs_mean'] = np.abs(x).mean()\n        X_test.loc[seg_id, 'abs_std'] = np.abs(x).std()        \n        \n#         # Look at some chunkks of the current segment\n#         X_test.loc[seg_id, 'first_50k_ave'] = x[:50000].mean()\n#         X_test.loc[seg_id, 'first_50k_std'] = x[:50000].std()\n#         X_test.loc[seg_id, 'first_50k_max'] = x[:50000].max()\n#         X_test.loc[seg_id, 'first_50k_min'] = x[:50000].min()\n#         X_test.loc[seg_id, 'first_50k_q95'] = np.quantile(x[:50000],0.95)\n#         X_test.loc[seg_id, 'first_50k_q99'] = np.quantile(x[:50000],0.99)\n#         X_test.loc[seg_id, 'first_50k_q05'] = np.quantile(x[:50000],0.05)\n#         X_test.loc[seg_id, 'first_50k_q01'] = np.quantile(x[:50000],0.01)\n#         X_test.loc[seg_id, 'first_50k_kurtosis'] = kurtosis(x[:50000], bias=False)\n#         X_test.loc[seg_id, 'first_50k_variance'] = np.var(x[:50000])\n#         X_test.loc[seg_id, 'first_50k_skew'] = skew(x[:50000])    \n#         X_test.loc[seg_id, 'first_50k_median'] = np.median(x[:50000])\n#         X_test.loc[seg_id, 'first_50k_mad'] = np.mean(np.abs(x[:50000] - x[:50000].mean()))    \n#         X_test.loc[seg_id, 'first_50k_abs_mean'] = np.abs(x[:50000]).mean()\n#         X_test.loc[seg_id, 'first_50k_abs_std'] = np.abs(x[:50000]).std()\n\n#         X_test.loc[seg_id, 'middle_50k_ave'] = x[50000:100000].mean()\n#         X_test.loc[seg_id, 'middle_50k_std'] = x[50000:100000].std()\n#         X_test.loc[seg_id, 'middle_50k_max'] = x[50000:100000].max()\n#         X_test.loc[seg_id, 'middle_50k_min'] = x[50000:100000].min()\n#         X_test.loc[seg_id, 'middle_50k_q95'] = np.quantile(x[50000:100000],0.95)\n#         X_test.loc[seg_id, 'middle_50k_q99'] = np.quantile(x[50000:100000],0.99)\n#         X_test.loc[seg_id, 'middle_50k_q05'] = np.quantile(x[50000:100000],0.05)\n#         X_test.loc[seg_id, 'middle_50k_q01'] = np.quantile(x[50000:100000],0.01)\n#         X_test.loc[seg_id, 'middle_50k_kurtosis'] = kurtosis(x[50000:100000], bias=False)\n#         X_test.loc[seg_id, 'middle_50k_variance'] = np.var(x[50000:100000])\n#         X_test.loc[seg_id, 'middle_50k_skew'] = skew(x[50000:100000])    \n#         X_test.loc[seg_id, 'middle_50k_median'] = np.median(x[50000:100000])\n#         X_test.loc[seg_id, 'middle_50k_mad'] = np.mean(np.abs(x[50000:100000] - x[50000:100000].mean()))    \n#         X_test.loc[seg_id, 'middle_50k_abs_mean'] = np.abs(x[50000:100000]).mean()\n#         X_test.loc[seg_id, 'middle_50k_abs_std'] = np.abs(x[50000:100000]).std()\n\n#         X_test.loc[seg_id, 'last_50k_ave'] = x[100000:150000].mean()\n#         X_test.loc[seg_id, 'last_50k_std'] = x[100000:150000].std()\n#         X_test.loc[seg_id, 'last_50k_max'] = x[100000:150000].max()\n#         X_test.loc[seg_id, 'last_50k_min'] = x[100000:150000].min()\n#         X_test.loc[seg_id, 'last_50k_q95'] = np.quantile(x[100000:150000],0.95)\n#         X_test.loc[seg_id, 'last_50k_q99'] = np.quantile(x[100000:150000],0.99)\n#         X_test.loc[seg_id, 'last_50k_q05'] = np.quantile(x[100000:150000],0.05)\n#         X_test.loc[seg_id, 'last_50k_q01'] = np.quantile(x[100000:150000],0.01)\n#         X_test.loc[seg_id, 'last_50k_kurtosis'] = kurtosis(x[100000:150000], bias=False)\n#         X_test.loc[seg_id, 'last_50k_variance'] = np.var(x[100000:150000])\n#         X_test.loc[seg_id, 'last_50k_skew'] = skew(x[100000:150000])    \n#         X_test.loc[seg_id, 'last_50k_median'] = np.median(x[100000:150000])\n#         X_test.loc[seg_id, 'last_50k_mad'] = np.mean(np.abs(x[100000:150000] - x[100000:150000].mean()))    \n#         X_test.loc[seg_id, 'last_50k_abs_mean'] = np.abs(x[100000:150000]).mean()\n#         X_test.loc[seg_id, 'last_50k_abs_std'] = np.abs(x[100000:150000]).std()\n\n#         X_test.loc[seg_id, 'last_bin_ave'] = x[145904:150000].mean()\n#         X_test.loc[seg_id, 'last_bin_std'] = x[145904:150000].std()\n#         X_test.loc[seg_id, 'last_bin_max'] = x[145904:150000].max()\n#         X_test.loc[seg_id, 'last_bin_min'] = x[145904:150000].min()\n#         X_test.loc[seg_id, 'last_bin_q95'] = np.quantile(x[145904:150000],0.95)\n#         X_test.loc[seg_id, 'last_bin_q99'] = np.quantile(x[145904:150000],0.99)\n#         X_test.loc[seg_id, 'last_bin_q05'] = np.quantile(x[145904:150000],0.05)\n#         X_test.loc[seg_id, 'last_bin_q01'] = np.quantile(x[145904:150000],0.01)\n#         X_test.loc[seg_id, 'last_bin_kurtosis'] = kurtosis(x[145904:150000], bias=False)\n#         X_test.loc[seg_id, 'last_bin_variance'] = np.var(x[145904:150000])\n#         X_test.loc[seg_id, 'last_bin_skew'] = skew(x[145904:150000])    \n#         X_test.loc[seg_id, 'last_bin_median'] = np.median(x[145904:150000])\n#         X_test.loc[seg_id, 'last_bin_mad'] = np.mean(np.abs(x[145904:150000] - x[100000:150000].mean()))    \n#         X_test.loc[seg_id, 'last_bin_abs_mean'] = np.abs(x[145904:150000]).mean()\n#         X_test.loc[seg_id, 'last_bin_abs_std'] = np.abs(x[145904:150000]).std()\n\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e53c603b1508dfbf3a7673a901f8d23edf99d82d"
      },
      "cell_type": "code",
      "source": "X_test_scaled = scaler.transform(X_test)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "68b61d8801c80962ca733cfb13b2eded9900201b"
      },
      "cell_type": "code",
      "source": "X_test_scaled.shape",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "4fc6c266a0e9d288a64d784bb717be45f72ee159"
      },
      "cell_type": "markdown",
      "source": "Predict and save models predictions to a file. Best performer of this 3 models is the SVM Regressor. Commented code to only submit one file to the challenge."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b9303b72c685a86f0aec376318bf85f340804d77"
      },
      "cell_type": "code",
      "source": "# import the modules we'll need\nfrom IPython.display import HTML\nimport pandas as pd\nimport numpy as np\nimport base64\n\n# function that takes in a dataframe and creates a text link to  \n# download it (will only work for files < 2MB or so)\ndef create_download_link(df, title = \"Download CSV file\", filename = \"submission.csv\"):  \n    csv = df.to_csv()\n    b64 = base64.b64encode(csv.encode())\n    payload = b64.decode()\n    html = '<a download=\"{filename}\" href=\"data:text/csv;base64,{payload}\" target=\"_blank\">{title}</a>'\n    html = html.format(payload=payload,title=title,filename=filename)\n    return HTML(html)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2813dc3611616de568d8189350f7a6d195173897"
      },
      "cell_type": "code",
      "source": "# SVM Predictions\nsvm_prediction = svm.predict(X_test_scaled)\nsubmission['time_to_failure'] = svm_prediction\nsubmission.to_csv('svm_submission.csv')\n\n\n# create a link to download the dataframe\ncreate_download_link(submission, filename = 'svm_submission.csv')\n\n# ↓ ↓ ↓  Yay, download link! ↓ ↓ ↓ ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "927d701a7678db1eb3d743da314b4ba82e521616"
      },
      "cell_type": "code",
      "source": "# svm_prediction = svm.predict(X_test_scaled)\n# svm_prediction\n# svm_pred = svm.predict(X_test_scaled)\n# svm_pred",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "309ed67f81639500adecdf6896926467a27a2378"
      },
      "cell_type": "code",
      "source": "# XGB Predictions\nxgb_prediction = xgb.predict(X_test_scaled)\nsubmission['time_to_failure'] = xgb_prediction\nsubmission.to_csv('xgb_submission.csv')\n\n# create a link to download the dataframe\ncreate_download_link(submission, filename = 'xgb_submission.csv')\n\n# ↓ ↓ ↓  Yay, download link! ↓ ↓ ↓ ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4a6a60321dacea47ace19f80f78a3f4a4c842955"
      },
      "cell_type": "code",
      "source": "# # RF Predictions\n# rf_prediction = rf.predict(X_test_scaled)\n# submission['time_to_failure'] = rf_prediction\n# submission.to_csv('rf_submission.csv')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e64c64838e11352862c6362f97d33c4e9ec53595"
      },
      "cell_type": "code",
      "source": "y_train = train['time_to_failure']",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2ae8343cdd3b8d3edea48539a39b745ce7011b85"
      },
      "cell_type": "code",
      "source": "y_train.mean()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "41b70b44f78ea7ce3fbe2b407890a0888287718b"
      },
      "cell_type": "code",
      "source": "submission = pd.read_csv('../input/sample_submission.csv', index_col='seg_id')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "79607e9d0829a80d9d6375a78696e99ffa16e45e"
      },
      "cell_type": "code",
      "source": "submission['time_to_failure'] = y_train.mean()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3da0ee74c1595014b94eb95f731bc58f6c1696a2"
      },
      "cell_type": "code",
      "source": "# SVM Predictions\nsubmission.to_csv('dummy_submission.csv')\n\n\n# create a link to download the dataframe\ncreate_download_link(submission, filename = 'dummy_submission.csv')\n\n# ↓ ↓ ↓  Yay, download link! ↓ ↓ ↓ ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3bfdfecfc143139b6e0725d46775546bdc854a7a"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e47829b573d128359fe666eda62de3dcc268473c"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}